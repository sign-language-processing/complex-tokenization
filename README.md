# Tokenization for Complex Scripts

This repository decomposes complex scripts (such as SignWriting and Chinese) into
smaller units, and proposes more effective tokenization for NLP tasks.

## Usage

Install:

```bash
git clone https://github.com/sign-language-processing/complex-tokenization.git
cd complex-tokenization
pip install ".[dev]"
```

Pretokenize text using a Huggingface Tokenizer implementation:

```python
from complex_tokenization.tokenizer import WordsSegmentationTokenizer

pretokenizer = WordsSegmentationTokenizer(max_bytes=16)
tokens = pretokenizer.tokenize("hello world! æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨ ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦")
# ['hello ', 'world! ', 'æˆ‘', 'çˆ±', 'åŒ—äº¬', 'å¤©å®‰é—¨', ' ', 'ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦â€']
```

## Cite

If you use this code in your research, please consider citing the work:

```bibtex
@misc{moryossef2025complex,
  title={Tokenization for Complex Scripts},
  author={Moryossef, Amit},
  howpublished={\url{https://github.com/sign-language-processing/complex-tokenization}},
  year={2025}
}
```